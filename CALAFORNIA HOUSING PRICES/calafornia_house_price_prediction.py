# -*- coding: utf-8 -*-
"""Calafornia House price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vF2fypOwiGAce2t1_4vbR81udqPdjDR

**Introduction for the California Housing Price Prediction Project**

In this project, we build and compare multiple machine learning models to predict housing prices in California. Using Python and libraries like Pandas, NumPy, Scikit-learn, and TensorFlow/Keras, to analyze California housing dataset.This dataset includes features such as geographical coordinates, housing attributes, and proximity to the ocean.The target variable is the median housing price in various districts.

The primary objective is to explore regression techniques to predict continuous values effectively. The project demonstrates the end-to-end machine learning workflow, including data preprocessing, feature scaling, model selection, and evaluation.

The goal of this project is to predict median house prices in California using various machine learning models. We experiment with traditional regression models and neural networks, comparing their performance to determine the best approach

**IMPORTING NECESSARY LIBRARIES AND DEPENDENCIES**


These libraries are essential for data handling (Pandas), numerical computations (NumPy), and data visualization (Matplotlib, Seaborn).Also  We import machine learning models (LinearRegression, RandomForest, etc.), preprocessing tools (StandardScaler), and evaluation metrics (mean_squared_error) from Scikit-learn.While TensorFlow/Keras is used to design, train, and save neural networks.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import mean_squared_error as mse
from sklearn.neighbors import KNeighborsRegressor

"""Loading the Dataset"""

housing_pd = pd.read_csv('/content/housing.csv')

housing_pd

#To understand the distribution of the categorical variable ocean_proximity.
housing_pd['ocean_proximity'].value_counts()

"""**Shuffling and Encoding categorical variables**

Shuffling ensures randomness in the dataset. Encoding converts ocean_proximity into numerical "dummy variables," which machine learning models can process.
"""

housing_pd_shuffled = housing_pd.sample(n=len(housing_pd), random_state=1)
housing_pd_shuffled

# Preprocess ocean_proximity
housing_pd_shuffled = housing_pd.sample(n=len(housing_pd), random_state=1)
housing_pd_final = pd.concat(
    [housing_pd_shuffled.drop('ocean_proximity', axis=1),
     pd.get_dummies(housing_pd_shuffled['ocean_proximity'])],
    axis=1
)

# Rearrange columns
housing_pd_final = housing_pd_final[[
    'longitude', 'latitude', 'housing_median_age', 'total_rooms',
    'total_bedrooms', 'population', 'households', 'median_income',
    '<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN', 'median_house_value']]

housing_pd_final

"""**Handeling Missing Values**"""

housing_pd_final = housing_pd_final.dropna()

len(housing_pd_final)

"""**Splitting the data **

Here Data is divided into:

1)Training set: Used to train the model.


2)Validation set: Used to fine-tune hyperparameters.


3)Test set: Used to evaluate the final model's performance.
"""

train_pd, test_pd, val_pd = housing_pd_final[:18000], housing_pd_final[18000:19217], housing_pd_final[19215:]
len(train_pd), len(test_pd), len(val_pd)

X_train, y_train = train_pd.to_numpy()[:, :-1], train_pd.to_numpy()[:, -1]
X_val, y_val = val_pd.to_numpy()[:, :-1], val_pd.to_numpy()[:, -1]
X_test, y_test = test_pd.to_numpy()[:, :-1], test_pd.to_numpy()[:, -1]

X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape

"""**Feature Scaling
& Custom Processor**

Scaling ensures all numerical features have a similar range, preventing dominance of features with larger values.

while Preprocessor applies scaling consistently to training, validation, and test sets, ensuring uniform transformation.
"""

scaler = StandardScaler().fit(X_train[:, :8])

def preprocessor(X):
  A = np.copy(X)
  A[:, :8] = scaler.transform(A[:, :8])
  return A

X_train, X_val, X_test = preprocessor(X_train), preprocessor(X_val), preprocessor(X_test)

X_train.shape, X_val.shape, X_test.shape



pd.DataFrame(X_train).head()

pd.DataFrame(X_train)[0].hist()

"""**A)using Linear Regression**


A simple model that predicts house prices by fitting a straight line through the data which May underperform with complex data or non-linear relationships.
"""

lm = LinearRegression().fit(X_train, y_train)
mse(lm.predict(X_train), y_train, squared=False), mse(lm.predict(X_val), y_val, squared=False)

"""**B)K-Nearest Neighbors(KNN)**

Predicts prices based on the average values of the nearest 10 data points.
Limitation: Can overfit if the number of neighbors is too small.
"""

knn = KNeighborsRegressor(n_neighbors=10).fit(X_train, y_train)
mse(knn.predict(X_train), y_train, squared=False), mse(knn.predict(X_val), y_val, squared=False)

"""**C)Random Forest**


Uses multiple decision trees to make predictions and Handles non-linear data well and reduces overfitting via ensemble learning.
"""

rfr = RandomForestRegressor(max_depth=10).fit(X_train, y_train)
mse(rfr.predict(X_train), y_train, squared=False), mse(rfr.predict(X_val), y_val, squared=False)

"""**D)Gradient Boosting**

Combines weak learners incrementally to improve accuracy. Typically outperforms Random Forest in predictive tasks.
"""

gbr = GradientBoostingRegressor(n_estimators=250).fit(X_train, y_train)
mse(gbr.predict(X_train), y_train, squared=False), mse(gbr.predict(X_val), y_val, squared=False)

# Convert data to float32
X_train = X_train.astype('float32')
X_val = X_val.astype('float32')
y_train = y_train.astype('float32')
y_val = y_val.astype('float32')

"""IMPORTING NECESSARY LIBRARIES AND DEPENDENCIES.

1)for Building Simple Neural Network**

 A basic neural network with one hidden layer and two neurons.
"""

from tensorflow.keras.models import Sequential,load_model
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

simple_nn = Sequential()
simple_nn.add(InputLayer((13,)))
simple_nn.add(Dense(2, 'relu'))
simple_nn.add(Dense(1, 'linear'))

opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/simple_nn.keras', save_best_only=True)  #save as .keras format
simple_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
simple_nn.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), callbacks=[cp], epochs=100)

simple_nn = load_model('models/simple_nn.keras')
mse(simple_nn.predict(X_train), y_train, squared=False), mse(simple_nn.predict(X_val), y_val, squared=False)

"""**2)for Medium Neural Network**

Adds more complexity with two hidden layers (32 and 16 neurons)
"""

medium_nn = Sequential()
medium_nn.add(InputLayer((13,)))
medium_nn.add(Dense(32, 'relu'))
medium_nn.add(Dense(16, 'relu'))
medium_nn.add(Dense(1, 'linear'))

opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/medium_nn.keras', save_best_only=True)
medium_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
medium_nn.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), callbacks=[cp], epochs=100)

medium_nn = load_model('models/medium_nn.keras')
mse(medium_nn.predict(X_train), y_train, squared=False), mse(medium_nn.predict(X_val), y_val, squared=False)

"""**3) for Large Neural Network**


A highly complex network with multiple layers, useful for capturing intricate patterns.
"""

large_nn = Sequential()
large_nn.add(InputLayer((13,)))
large_nn.add(Dense(256, 'relu'))
large_nn.add(Dense(128, 'relu'))
large_nn.add(Dense(64, 'relu'))
large_nn.add(Dense(32, 'relu'))
large_nn.add(Dense(1, 'linear'))

#Training Neural Network
#Neural networks require iterative optimization, where the optimizer (e.g., Adam) adjusts weights to minimize error.
opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/large_nn.keras', save_best_only=True)
large_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
large_nn.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), callbacks=[cp], epochs=100)

large_nn = load_model('models/large_nn.keras')
mse(large_nn.predict(X_train), y_train, squared=False), mse(large_nn.predict(X_val), y_val, squared=False)

mse(gbr.predict(X_test), y_test, squared=False)

"""**Evaluation**


Models are evaluated using the Root Mean Squared Error (RMSE):

1)Linear Regression: Poor performance due to limited complexity.


2)KNN: Improved accuracy but risks overfitting with fewer neighbors.


3)Random Forest: Strong performance with manageable complexity.

4)Gradient Boosting: Best among traditional models, achieving low RMSE.


5)Neural Networks: Increasing complexity improves accuracy, but the risk of overfitting rises with deeper architectures.

**CONCLUSION**


The **most suitable Model** here is **Gradient Boosting **achieved the best overall performance on the test set with the **lowest RMSE (~49,000).**


while
**Simpler models (like Linear Regression)** are **less effective** for complex datasets.

Ensemble methods like **Random Forest and Gradient Boosting** excel in **capturing non-linear relationships.**
and

Neural networks require careful tuning to balance complexity and overfitting.
This project highlights the importance of trying multiple models, preprocessing data effectively, and evaluating results to make informed decisions.
"""