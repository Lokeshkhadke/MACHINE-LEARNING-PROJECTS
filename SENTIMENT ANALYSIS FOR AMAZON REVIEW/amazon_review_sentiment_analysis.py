# -*- coding: utf-8 -*-
"""Amazon Review Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PxN8QrilmOn9_LiimrHsLF-0Rb3mIzM8

The project focuses on creating a sentiment analysis system and a basic recommendation system using Amazon product reviews. The goal is to analyze the emotions expressed in customer reviews (positive or negative) and predict sentiments using machine learning techniques. To achieve this, we preprocess the data, visualize common words, apply sentiment analysis with the Vader Sentiment Analyzer, and build a classification model using Logistic Regression. The project also includes text vectorization using TF-IDF to convert text into numerical features that the model can understand. Additionally, we demonstrate how a recommendation system can provide personalized suggestions based on user preferences.
"""

import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""**Library Installation & Import**



"""

!pip install nltk
!pip install textblob
!pip install wordcloud

"""**IMPORTING NECESSARY LIBRARIES**"""

import matplotlib.pyplot as plt
from warnings import filterwarnings
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import classification_report
from textblob import Word, TextBlob
from wordcloud import WordCloud
import string
import nltk

"""**libraries used and their use **

**Pandas and NumPy:** For data handling and manipulation.

**Matplotlib**: For visualization (e.g., word clouds).

**nltk (Natural Language Toolkit):**
Provides tools like stopwords, SentimentIntensityAnalyzer, and tokenization.

**TextBlob:** For text lemmatization (simplifying words to their base form).

**WordCloud:** To create visualizations of text data.

**LogisticRegression:** A classification algorithm for sentiment prediction.

**TfidfVectorizer:** To convert text into numerical features for the model.
"""

nltk.download("stopwords")
nltk.download("punkt")
nltk.download("wordnet")
nltk.download("vader_lexicon")
nltk.download('punkt_tab')

"""**IMPORTING DATASETS**"""

train = pd.read_csv("/content/train.csv")
test = pd.read_csv("/content/test.csv")

"""RENAMING COLUMN"""

train.columns = ["polarity","title", "text"]
train.drop(["polarity"],
           axis=1,
           inplace = True)
train = train.sample(300000,
                     random_state = 99)  #TOOK A SAMPLE FOR FASTER PROCESSING.
train.head()

"""in above step The dataset is loaded into Pandas DataFrames for easier manipulation.
and Columns are renamed for clarity: title and text represent the review title and body while
Sampling 300,000 rows is done to reduce computational load.
"""

train.shape

test.columns = ["polarity","title", "text"]
test.drop(["polarity"],
          axis=1,
          inplace = True)
test = test.sample(100000,
                   random_state = 99)
test.head()

test.shape

print("train:\n",
      train.isnull().sum(),
      "\n",
      "----------\n")
print("test:\n",
      test.isnull().sum(),
      "\n",
      "----------\n")

"""**HANDELING MISSING VALUES**

Missing values in the title column are replaced with "no title" to ensure no null values disrupt processing.

"""

train["title"] = train["title"].fillna("no title")
test["title"] = test["title"].fillna("no title")

print("train:\n",
      train.isnull().sum(),
      "\n",
      "----------\n")
print("test:\n",
      test.isnull().sum(),
      "\n",
      "----------\n")

"""**PREPROCESSING THE DATA**"""

#LOWERCASING THE TEXT.
for col in train.columns:
    train[col] = train[col].str.lower()

for col in test.columns:
    test[col] = test[col].str.lower()

"""Text is converted to lowercase for consistency and to prevent issues like treating "Good" and "good" as different words."""

#REMOVE PUNCTUATIONS.
def Remove_punctuation(text):
    return text.translate(str.maketrans("",
                                        "",
                                        string.punctuation))

for col in train.columns:
    train[col] = train[col].apply(Remove_punctuation)

for col in test.columns:
    test[col] = test[col].apply(Remove_punctuation)

"""Removing punctuation ensures only meaningful words remain for analysis.

REMOVING STOPWORDS
"""

stop_words = stopwords.words('english')
for col in train.columns:
    train[col] = train[col].apply(lambda x: " ".join(x for x in str(x).split() if x not in stop_words))

for col in test.columns:
    test[col] = test[col].apply(lambda x: " ".join(x for x in str(x).split() if x not in stop_words))

"""Stopwords like "is", "the", and "and" are common words that don't add meaning to sentiment analysis and are removed."""

train.head()

"""**Lemmatization :** Lemmatization reduces words to their base forms, e.g., "running" becomes "run," making analysis more consistent."""

#train
for col in train.columns:
    train[col] = train[col].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

#test
for col in test.columns:
    test[col] = test[col].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))

train.head()

test.head()

"""**CREATING VISUALIZATION ** : WORDCLOUD"""

all_text = ' '.join(train["text"]).lower()

wordcloud = WordCloud().generate(all_text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()


wordcloud = WordCloud(max_font_size=50,
                      max_words=100,
                      background_color="white").generate(all_text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

wordcloud.to_file("wordcloud.png")

"""**SENTIMENT ANALYSIS WITH Vader** :

Vader Sentiment Analyzer computes a sentiment score (compound) for each text. A positive score indicates positive sentiment, and a negative score indicates negative sentiment.
The label_text column creates a binary sentiment label: 1 (positive) or 0 (negative).
"""

sent = SentimentIntensityAnalyzer()

train["score_title"] = train["title"].apply(lambda x: sent.polarity_scores(x)["compound"])
train["score_text"] = train["text"].apply(lambda x: sent.polarity_scores(x)["compound"])

test["score_title"] = test["title"].apply(lambda x: sent.polarity_scores(x)["compound"])
test["score_text"] = test["text"].apply(lambda x: sent.polarity_scores(x)["compound"])

train["label_title"] = train["title"].apply(lambda x: 1 if sent.polarity_scores(x)["compound"] > 0 else 0)
train["label_text"] = train["text"].apply(lambda x: 1 if sent.polarity_scores(x)["compound"] > 0 else 0)

test["label_title"] = test["title"].apply(lambda x: 1 if sent.polarity_scores(x)["compound"] > 0 else 0)
test["label_text"] = test["text"].apply(lambda x: 1 if sent.polarity_scores(x)["compound"] > 0 else 0)

train["label_text"].value_counts()

test["label_text"].value_counts()

train.head()

test.head()

"""**TEXT VECTORIZER**"""

y_train = train["label_text"]
X_train = train["text"]

y_test = test["label_text"]
X_test = test["text"]

"""**Text Vectorization :**

TF-IDF (Term Frequency-Inverse Document Frequency) converts text into numerical vectors. This highlights important words while downweighting frequent, less informative words.
"""

vectorizer = TfidfVectorizer().fit(X_train)
x_train_tfidf = vectorizer.transform(X_train)
x_test_tfidf = vectorizer.transform(X_test)

"""**Modelling (Logistic Regression)**
Logistic Regression is a simple yet effective algorithm for binary classification problems like sentiment prediction.
The classification_report evaluates precision, recall, and F1-score for the model's predictions.
"""

model = LogisticRegression().fit(x_train_tfidf,
                                 y_train)
y_predict = model.predict(x_test_tfidf)
print(classification_report(y_predict,
                            y_test))

"""**Evaluation and Predictions**


CROSS-VALIDATION : To ensures the model generalizes well to unseen data.

"""

cross_val_score(model,
                x_test_tfidf,
                y_test,
                cv=5).mean()

"""Predict Sentiment for a Random Review to
Demonstrates how the model predicts sentiment for individual reviews.
"""

random_text = pd.Series(train["text"].sample(1).values)
new_text = CountVectorizer().fit(X_train).transform(random_text)
prediction = model.predict(new_text)
print(f'Text: {random_text[0]} \n Prediction: {prediction}')

out = pd.DataFrame(y_predict, columns = ["prediction"])

out.head(20)